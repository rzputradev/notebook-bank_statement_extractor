{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "expected_headers = [\n",
    "    {\"Posting\", \"Date\", \"Effective\", \"Branch\", \"Journal\", \"Transaction\", \"Description\", \"Amount\", \"DB/CR\", \"Balance\"},\n",
    "    {\"Tgl\", \"Transaksi\", \"No.\", \"Dokumen\", \"Uraian\", \"Tipe\", \"Mutasi\", \"Saldo\"},\n",
    "    {\"TIME\", \"REMARK\", \"DEBET\", \"CREDIT\", \"TELLER\", \"ID\"},\n",
    "    {\"Tanggal\", \"Keterangan\", \"Debit\", \"Kredit\", \"Saldo\", \"SEQ\"},\n",
    "    {\"Posting\", \"Date\", \"Remark\", \"Reference\", \"No\", \"Debit\", \"Credit\", \"Balance\"},\n",
    "    {\"Date\", \"Val.Date\", \"Description\", \"Reference\", \"No.\", \"Debet\", \"Credit\", \"Balance\"},\n",
    "    {\"Date\", \"Time\", \"Value\", \"Description\", \"Reference\", \"No.\", \"Debit\", \"Credit\", \"Saldo\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_table_header(json_data, page_idx):\n",
    "    blocks = json_data[\"pages\"][page_idx][\"blocks\"]\n",
    "    \n",
    "    found_words = {}\n",
    "    \n",
    "    for block in blocks:\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for word in line.get(\"words\", []):\n",
    "                value = word[\"value\"]\n",
    "                found_words[value] = found_words.get(value, []) + [word[\"geometry\"]]\n",
    "    \n",
    "    for header in expected_headers:\n",
    "        matched_count = sum(1 for word in header if word in found_words)\n",
    "        if matched_count >= len(header) * 0.8: \n",
    "            matched_geometries = {word: found_words[word] for word in header if word in found_words}\n",
    "            return matched_geometries\n",
    "    \n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_header_coordinate(matched_geometries, y_tolerance=0.01):\n",
    "    rows = {}\n",
    "    \n",
    "    for word, geometries in matched_geometries.items():\n",
    "        for geometry in geometries:\n",
    "            y_coord = geometry[0][1]\n",
    "            matched_row = None\n",
    "            for existing_y in rows.keys():\n",
    "                if abs(existing_y - y_coord) < y_tolerance:\n",
    "                    matched_row = existing_y\n",
    "                    break\n",
    "            \n",
    "            if matched_row is None:\n",
    "                rows[y_coord] = []\n",
    "            else:\n",
    "                y_coord = matched_row\n",
    "                \n",
    "            rows[y_coord].append({\n",
    "                'word': word,\n",
    "                'geometry': geometry\n",
    "            })\n",
    "    \n",
    "    header_row = max(rows.items(), key=lambda x: len(x[1]))\n",
    "    \n",
    "    header_row_sorted = sorted(header_row[1], key=lambda x: x['geometry'][0][0])\n",
    "    words_in_order = [item['word'] for item in header_row_sorted]\n",
    "    geometries = [item['geometry'][0][1] for item in header_row_sorted]\n",
    "    highest_geometry = max(geometries)\n",
    "    lowest_geometry = min(geometries)\n",
    "    \n",
    "    return {\n",
    "        'highest_geometry': highest_geometry,\n",
    "        'lowest_geometry': lowest_geometry,\n",
    "        'words': words_in_order, #show words for debugging\n",
    "        'geometries': geometries #show geometries for debugging\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Keterangan': [[[0.21151358981092439, 0.296875],\n",
       "   [0.275008206407563, 0.3095703125]]],\n",
       " 'Kredit': [[[0.4944787289915966, 0.296875],\n",
       "   [0.5303669905462185, 0.3076171875]]],\n",
       " 'Saldo': [[[0.5938616071428572, 0.296875],\n",
       "   [0.6269892331932774, 0.3076171875]]],\n",
       " 'Debit': [[[0.37853203781512607, 0.296875],\n",
       "   [0.4102793461134454, 0.3076171875]]],\n",
       " 'SEQ': [[[0.802289587710084, 0.296875], [0.8298959427521009, 0.30859375]]],\n",
       " 'Tanggal': [[[0.34816504726890757, 0.1376953125],\n",
       "   [0.39647616859243695, 0.1513671875]],\n",
       "  [[0.0886653098739496, 0.296875], [0.13559611344537814, 0.310546875]]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'sample/scanned/bri-statement_transaction.json'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "table_header = find_table_header(json_data, 0) \n",
    "table_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_json_data(json_data, lowest_geometry, highest_geometry, page_idx):\n",
    "    above_header = []\n",
    "    below_header = []\n",
    "\n",
    "    blocks = json_data[\"pages\"][page_idx][\"blocks\"]\n",
    "    \n",
    "    for block in blocks:\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for word in line.get(\"words\", []):\n",
    "                    word_y_coord = word['geometry'][0][1]\n",
    "                    word_x_coord = word['geometry'][0][0]\n",
    "                    \n",
    "                    if word_y_coord < lowest_geometry:\n",
    "                        above_header.append({\n",
    "                            'value': word['value'],\n",
    "                            'y_coord': word_y_coord,\n",
    "                            'x_coord': word_x_coord\n",
    "                        })\n",
    "                    elif word_y_coord > highest_geometry:\n",
    "                        below_header.append({\n",
    "                            'value': word['value'],\n",
    "                            'confidence': word['confidence'],\n",
    "                            'geometry': word['geometry'],\n",
    "                        })\n",
    "\n",
    "    return above_header, below_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statement_id(above_header):\n",
    "    values = {item['value'] for item in above_header}\n",
    "\n",
    "    with open(\"database/statement_settings.json\") as s:\n",
    "        statement_settings = json.load(s)\n",
    "        \n",
    "    statement_id = None\n",
    "    lower_limit = None\n",
    "\n",
    "    for setting in statement_settings:\n",
    "        match_statement_count = sum(1 for value in setting[\"report_type\"] if value in values)\n",
    "        if match_statement_count == len(setting[\"report_type\"]):\n",
    "            statement_id = setting[\"id\"]\n",
    "            lower_limit = setting[\"lower_limit\"]\n",
    "\n",
    "    if statement_id:\n",
    "        return statement_id, lower_limit\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_content(below_header, lower_limit, tolerance=0.05):\n",
    "    rows = []\n",
    "    table_content = []\n",
    "    matched_items = [item for item in below_header if item['value'] in lower_limit]\n",
    "\n",
    "    if matched_items:\n",
    "        for mached_item in matched_items:\n",
    "            item_y = mached_item['geometry'][0][1]\n",
    "            found_row = False\n",
    "            for row in rows:\n",
    "                if abs(row[0]['geometry'][0][1] - item_y) <= tolerance:\n",
    "                    row.append(mached_item)\n",
    "                    found_row = True\n",
    "                    break\n",
    "            if not found_row:\n",
    "                rows.append([mached_item])\n",
    "\n",
    "        max_row = max(rows, key=len)\n",
    "        limit_coordinate = min(max_row, key=lambda item: item['geometry'][0][1])\n",
    "        limit_coordinate = limit_coordinate['geometry'][0][1]\n",
    "\n",
    "        for words in below_header:\n",
    "            if words[\"geometry\"][0][1] < (limit_coordinate - 0.01):\n",
    "                table_content.append(words)\n",
    "\n",
    "        return table_content\n",
    "    \n",
    "    else:\n",
    "        return below_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main process to normalize data and save it into json file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to bri-statement_transaction.json\n"
     ]
    }
   ],
   "source": [
    "file_path = 'sample/scanned/bri-statement_transaction.json'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "table_headers, header_coordinates, above_headers, below_headers, table_contents = {}, {}, {}, {}, {}\n",
    "all_pages_data = []\n",
    "\n",
    "for page in json_data[\"pages\"]:\n",
    "    page_idx = page[\"page_idx\"]\n",
    "    words_without_header = []\n",
    "\n",
    "    table_header = find_table_header(json_data, page_idx) \n",
    "    table_headers[page_idx] = table_header #find table header by page index\n",
    "\n",
    "    if table_headers[0] is not None and table_header is not None:\n",
    "\n",
    "        header_coordinate = find_header_coordinate(table_headers[page_idx])\n",
    "        header_coordinates[page_idx] = header_coordinate #find header coordinate by page index\n",
    "\n",
    "        above_header, below_header = split_json_data(json_data, header_coordinates[page_idx]['lowest_geometry'], header_coordinates[page_idx]['highest_geometry'], page_idx)\n",
    "        above_headers[page_idx], below_headers[page_idx] = above_header, below_header\n",
    "        \n",
    "        if above_headers[0] != []:\n",
    "            statement_id, lower_limit = get_statement_id(above_headers[0])\n",
    "        elif above_header[0] == []:\n",
    "            print(\"Account Statement is unknown\")\n",
    "            break\n",
    "\n",
    "        if statement_id:\n",
    "            table_content =  get_table_content(below_headers[page_idx], lower_limit)\n",
    "            table_contents[page_idx] = table_content\n",
    "        else:\n",
    "            print(\"Account Statement is unknown\")\n",
    "            break\n",
    "    elif table_headers[0] is not None and table_header is None:\n",
    "        for block in page.get(\"blocks\", []):\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for word in line.get(\"words\", []):\n",
    "                    words_without_header.append({\n",
    "                            'value': word['value'],\n",
    "                            'confidence': word['confidence'],\n",
    "                            'geometry': word['geometry'],\n",
    "                        })\n",
    "\n",
    "        below_headers[page_idx] = words_without_header\n",
    "        table_content = get_table_content(below_headers[page_idx], lower_limit)\n",
    "        table_contents[page_idx] = table_content\n",
    "    else:\n",
    "        print(\"Input is invalid!\")\n",
    "        break\n",
    "\n",
    "    # save normalize process into json file\n",
    "    page_data = {\n",
    "        \"page_id\": page_idx,\n",
    "        \"statement_id\": statement_id,\n",
    "        \"words\": []  \n",
    "    }\n",
    "    page_data[\"words\"].extend(table_contents[page_idx])\n",
    "    all_pages_data.append(page_data)\n",
    "\n",
    "if all_pages_data != []:\n",
    "    folder_path = \"sample/normalized\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    output_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    with open(output_path, 'w') as json_file:\n",
    "        json.dump(all_pages_data, json_file, indent=4)\n",
    "\n",
    "    print(f'Data has been written to {filename}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manual_extractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
