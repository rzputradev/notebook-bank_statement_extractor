{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "expected_headers = [\n",
    "    {\"Posting\", \"Date\", \"Effective\", \"Branch\", \"Journal\", \"Transaction\", \"Description\", \"Amount\", \"DB/CR\", \"Balance\"},\n",
    "    {\"Tgl\", \"Transaksi\", \"No.\", \"Dokumen\", \"Uraian\", \"Tipe\", \"Mutasi\", \"Saldo\"},\n",
    "    {\"TIME\", \"REMARK\", \"DEBET\", \"CREDIT\", \"TELLER\", \"ID\"},\n",
    "    {\"Tanggal\", \"Keterangan\", \"Debit\", \"Kredit\", \"Saldo\", \"SEQ\"},\n",
    "    {\"Posting\", \"Date\", \"Remark\", \"Reference\", \"No\", \"Debit\", \"Credit\", \"Balance\"},\n",
    "    {\"Date\", \"Val.Date\", \"Description\", \"Reference\", \"No.\", \"Debet\", \"Credit\", \"Balance\"},\n",
    "    {\"Date\", \"Time\", \"Value\", \"Description\", \"Reference\", \"No.\", \"Debit\", \"Credit\", \"Saldo\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_table_header(json_data, page_idx):\n",
    "    blocks = json_data[\"pages\"][page_idx][\"blocks\"]\n",
    "    \n",
    "    found_words = {}\n",
    "    \n",
    "    for block in blocks:\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for word in line.get(\"words\", []):\n",
    "                value = word[\"value\"]\n",
    "                found_words[value] = found_words.get(value, []) + [word[\"geometry\"]]\n",
    "    \n",
    "    for header in expected_headers:\n",
    "        matched_count = sum(1 for word in header if word in found_words)\n",
    "        if matched_count >= len(header) * 0.7: \n",
    "            matched_geometries = {word: found_words[word] for word in header if word in found_words}\n",
    "            return matched_geometries\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_header_coordinate(matched_geometries, y_tolerance=0.01):\n",
    "    rows = {}\n",
    "    \n",
    "    for word, geometries in matched_geometries.items():\n",
    "        for geometry in geometries:\n",
    "            y_coord = geometry[0][1]\n",
    "            matched_row = None\n",
    "            for existing_y in rows.keys():\n",
    "                if abs(existing_y - y_coord) < y_tolerance:\n",
    "                    matched_row = existing_y\n",
    "                    break\n",
    "            \n",
    "            if matched_row is None:\n",
    "                rows[y_coord] = []\n",
    "            else:\n",
    "                y_coord = matched_row\n",
    "                \n",
    "            rows[y_coord].append({\n",
    "                'word': word,\n",
    "                'geometry': geometry\n",
    "            })\n",
    "    \n",
    "    header_row = max(rows.items(), key=lambda x: len(x[1]))\n",
    "    \n",
    "    header_row_sorted = sorted(header_row[1], key=lambda x: x['geometry'][0][0])\n",
    "    words_in_order = [item['word'] for item in header_row_sorted]\n",
    "    geometries = [item['geometry'][0][1] for item in header_row_sorted]\n",
    "    highest_geometry = max(geometries)\n",
    "    lowest_geometry = min(geometries)\n",
    "    \n",
    "    return {\n",
    "        'highest_geometry': highest_geometry,\n",
    "        'lowest_geometry': lowest_geometry,\n",
    "        'words': words_in_order, #show words for debugging\n",
    "        'geometries': geometries #show geometries for debugging\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_json_data(json_data, lowest_geometry, highest_geometry, page_idx):\n",
    "    above_header = []\n",
    "    below_header = []\n",
    "\n",
    "    blocks = json_data[\"pages\"][page_idx][\"blocks\"]\n",
    "    \n",
    "    for block in blocks:\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for word in line.get(\"words\", []):\n",
    "                    word_y_coord = word['geometry'][0][1]\n",
    "                    word_x_coord = word['geometry'][0][0]\n",
    "                    \n",
    "                    if word_y_coord < lowest_geometry:\n",
    "                        above_header.append({\n",
    "                            'value': word['value'],\n",
    "                            'y_coord': word_y_coord,\n",
    "                            'x_coord': word_x_coord\n",
    "                        })\n",
    "                    elif word_y_coord > highest_geometry:\n",
    "                        below_header.append({\n",
    "                            'value': word['value'],\n",
    "                            'confidence': word['confidence'],\n",
    "                            'geometry': word['geometry'],\n",
    "                        })\n",
    "\n",
    "    return above_header, below_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statement_id(above_header):\n",
    "    values = {item['value'] for item in above_header}\n",
    "\n",
    "    with open(\"database/statement_settings.json\") as s:\n",
    "        statement_settings = json.load(s)\n",
    "\n",
    "\n",
    "    for setting in statement_settings:\n",
    "        match_statement_count = sum(1 for value in setting[\"report_type\"] if value in values)\n",
    "        if match_statement_count == len(setting[\"report_type\"]):\n",
    "            statement_id = setting[\"id\"]\n",
    "            lower_limit = setting[\"lower_limit\"]\n",
    "            return statement_id, lower_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_content(below_header, lower_limit, tolerance=0.05):\n",
    "    rows = []\n",
    "    table_content = []\n",
    "    matched_items = [item for item in below_header if item['value'] in lower_limit]\n",
    "\n",
    "    if matched_items:\n",
    "        for mached_item in matched_items:\n",
    "            item_y = mached_item['geometry'][0][1]\n",
    "            found_row = False\n",
    "            for row in rows:\n",
    "                if abs(row[0]['geometry'][0][1] - item_y) <= tolerance:\n",
    "                    row.append(mached_item)\n",
    "                    found_row = True\n",
    "                    break\n",
    "            if not found_row:\n",
    "                rows.append([mached_item])\n",
    "\n",
    "        max_row = max(rows, key=len)\n",
    "        limit_coordinate = min(max_row, key=lambda item: item['geometry'][0][1])\n",
    "        limit_coordinate = limit_coordinate['geometry'][0][1]\n",
    "\n",
    "        for words in below_header:\n",
    "            if words[\"geometry\"][0][1] < (limit_coordinate - 0.01):\n",
    "                table_content.append(words)\n",
    "\n",
    "        return table_content\n",
    "    \n",
    "    else:\n",
    "        return below_header\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main process to normalize data and save it into json file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m above_headers[page_idx], below_headers[page_idx] \u001b[38;5;241m=\u001b[39m above_header, below_header \u001b[38;5;66;03m# split data \u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m above_headers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m []: \n\u001b[1;32m---> 24\u001b[0m     statement_id, lower_limit \u001b[38;5;241m=\u001b[39m get_statement_id(above_headers[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m above_header[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m []:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccount Statement is unknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "file_path = 'sample/scanned/mandiri-account_statement2.json'\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "table_headers, header_coordinates, above_headers, below_headers, table_contents = {}, {}, {}, {}, {}\n",
    "all_pages_data = []\n",
    "\n",
    "for page in json_data[\"pages\"]:\n",
    "    page_idx = page[\"page_idx\"]\n",
    "    words_without_header = []\n",
    "\n",
    "    table_header = find_table_header(json_data, page_idx) \n",
    "    table_headers[page_idx] = table_header #find table header by page index\n",
    "\n",
    "    if table_headers[0] is not None and table_header is not None:\n",
    "        header_coordinate = find_header_coordinate(table_headers[page_idx])\n",
    "        header_coordinates[page_idx] = header_coordinate #find header coordinate by page index\n",
    "\n",
    "        above_header, below_header = split_json_data(json_data, header_coordinates[page_idx]['lowest_geometry'], header_coordinates[page_idx]['highest_geometry'], page_idx)\n",
    "        above_headers[page_idx], below_headers[page_idx] = above_header, below_header # split data \n",
    "        \n",
    "        if above_headers[0] != []: \n",
    "            statement_id, lower_limit = get_statement_id(above_headers[0])\n",
    "        elif above_header[0] == []:\n",
    "            print(\"Account Statement is unknown\")\n",
    "            break\n",
    "\n",
    "        table_content =  get_table_content(below_headers[page_idx], lower_limit)\n",
    "        table_contents[page_idx] = table_content\n",
    "\n",
    "    elif table_headers[0] is not None and table_header is None:\n",
    "        for block in page.get(\"blocks\", []):\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for word in line.get(\"words\", []):\n",
    "                    words_without_header.append({\n",
    "                            'value': word['value'],\n",
    "                            'confidence': word['confidence'],\n",
    "                            'geometry': word['geometry'],\n",
    "                        })\n",
    "\n",
    "        below_headers[page_idx] = words_without_header\n",
    "        table_content = get_table_content(below_headers[page_idx], lower_limit)\n",
    "        table_contents[page_idx] = table_content\n",
    "    else:\n",
    "        print(\"Input is invalid!\")\n",
    "        break\n",
    "\n",
    "    # save normalize process into json file\n",
    "    page_data = {\n",
    "        \"page_id\": page_idx,\n",
    "        \"statement_id\": statement_id,\n",
    "        \"words\": []  \n",
    "    }\n",
    "    page_data[\"words\"].extend(table_contents[page_idx])\n",
    "    all_pages_data.append(page_data)\n",
    "\n",
    "if all_pages_data != []:\n",
    "    folder_path = \"sample/normalized\"\n",
    "    filename = os.path.basename(file_path)\n",
    "    output_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    with open(output_path, 'w') as json_file:\n",
    "        json.dump(all_pages_data, json_file, indent=4)\n",
    "\n",
    "    print(f'Data has been written to {filename}')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manual_extractor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
